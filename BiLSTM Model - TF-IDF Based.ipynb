{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0125c6",
   "metadata": {},
   "source": [
    "<h3> Training BiLSTM Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4978a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8035f",
   "metadata": {},
   "source": [
    "<h4> Loading all the data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8834a243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7349</th>\n",
       "      <th>7350</th>\n",
       "      <th>7351</th>\n",
       "      <th>7352</th>\n",
       "      <th>7353</th>\n",
       "      <th>7354</th>\n",
       "      <th>7355</th>\n",
       "      <th>7356</th>\n",
       "      <th>7357</th>\n",
       "      <th>7358</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64.037076</td>\n",
       "      <td>66.621906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.012165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.714798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.347074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147.918974</td>\n",
       "      <td>151.587765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.002897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.280848</td>\n",
       "      <td>56.115823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.259993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.509559</td>\n",
       "      <td>16.926467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.085699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.428494</td>\n",
       "      <td>20.762021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.752172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.832581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7359 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1    2    3    4    5    6          7    8    9  ...  \\\n",
       "0   64.037076   66.621906  0.0  0.0  0.0  0.0  0.0  21.012165  0.0  0.0  ...   \n",
       "1  147.918974  151.587765  0.0  0.0  0.0  0.0  0.0   5.002897  0.0  0.0  ...   \n",
       "2   53.280848   56.115823  0.0  0.0  0.0  0.0  0.0  17.259993  0.0  0.0  ...   \n",
       "3   16.509559   16.926467  0.0  0.0  0.0  0.0  0.0   4.085699  0.0  0.0  ...   \n",
       "4   20.428494   20.762021  0.0  0.0  0.0  0.0  0.0   3.752172  0.0  0.0  ...   \n",
       "\n",
       "       7349      7350      7351  7352      7353      7354      7355  7356  \\\n",
       "0  0.000000  0.000000  1.714798   0.0  1.347074  0.000000  0.000000   0.0   \n",
       "1  0.000000  0.000000  0.000000   0.0  0.000000  1.609438  1.609438   0.0   \n",
       "2  0.000000  0.000000  0.000000   0.0  0.000000  0.000000  0.000000   0.0   \n",
       "3  1.609438  0.000000  0.000000   0.0  0.000000  0.000000  0.000000   0.0   \n",
       "4  0.000000  1.832581  0.000000   0.0  0.000000  0.000000  0.000000   0.0   \n",
       "\n",
       "       7357      7358  \n",
       "0  1.609438  1.609438  \n",
       "1  0.000000  0.000000  \n",
       "2  1.609438  1.609438  \n",
       "3  0.000000  0.000000  \n",
       "4  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 7359 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = \"_v3\"\n",
    "disease_symptoms = pd.read_csv(f\"data/top_N_filtered{tag}.csv\")\n",
    "with open(f\"data/filtered_symptom_dict{tag}.csv\", 'r') as f:\n",
    "    symptom_dict = data = json.load(f)\n",
    "\n",
    "with open(f\"data/icd9_dict{tag}.csv\", 'r') as f:\n",
    "    icd9_dict = json.load(f)\n",
    "\n",
    "tfidf_weights = pd.read_csv(f\"data/weight_i_j{tag}.csv\")\n",
    "tfidf_weights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf45e64",
   "metadata": {},
   "source": [
    "<h3> Define Custom DataLoader </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18fc365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_SYMPTOMS = 50\n",
    "MAX_DISEASE = 50\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filename):        \n",
    "        # read in the data files\n",
    "        self.hadm_list = self.process_raw_data(filename)\n",
    "        \n",
    "        # TF-IDF Weights\n",
    "        self.tfidf_weights = pd.read_csv(f\"data/weight_i_j{tag}.csv\")\n",
    "        \n",
    "        # Symptom dictionary\n",
    "        with open(f\"data/filtered_symptom_dict{tag}.csv\", 'r') as f:\n",
    "            self.symptom_dict = json.load(f)\n",
    "        \n",
    "        with open(f\"data/icd9_dict{tag}.csv\", 'r') as f:\n",
    "            self.icd9_dict = json.load(f)\n",
    "        \n",
    "        \n",
    "    def process_raw_data(self, filename):\n",
    "        symptom_disease_data = pd.read_csv(filename)\n",
    "        hadm_list = []\n",
    "        hadm_id_map = {}\n",
    "        hadm_index = 0\n",
    "        # Collecting all records for one admission in one list\n",
    "        for index, record in symptom_disease_data.iterrows():\n",
    "            # print(f\"processing {index} - {record}\")\n",
    "            hadm_id = record['HADM_ID']            \n",
    "            if hadm_id not in hadm_id_map:\n",
    "                hadm_id_map[hadm_id] = hadm_index\n",
    "                hadm_index += 1\n",
    "                hadm_list.append([])\n",
    "\n",
    "            idx = hadm_id_map[hadm_id]\n",
    "            hadm_list[idx].append(record)\n",
    "        \n",
    "        return hadm_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hadm_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            Output:\n",
    "            symptpm_vector : max_number_of_symptoms (50) x number_of_diagnoses (50)\n",
    "            diagnoses_vector = number_of_diagnoses\n",
    "            symptom_count = number of symtoms for current record\n",
    "        \"\"\"\n",
    "        list_of_records = self.hadm_list[index]\n",
    "        symptom_string = list_of_records[0]['SYMPTOMS']\n",
    "        # print(f\"length of records : {len(list_of_records)}\")\n",
    "        symptom_vector = np.zeros((MAX_SYMPTOMS, MAX_DISEASE))\n",
    "        # print(f\"Processing item with symptom vector of : {symptom_string}\")\n",
    "        diag_vector = np.zeros((MAX_DISEASE))\n",
    "        symptom_list = self.create_symptom_vector(symptom_string, self.symptom_dict)\n",
    "        # print(f\"symptom_list : {symptom_list}\")\n",
    "        # Populate Symptom Vector by getting corresponding embeddings from TF-IDF vector\n",
    "        for index, symptom_idx in enumerate(symptom_list):\n",
    "            # print(f\"Symptom vector index: {index}, symptom index : {symptom_idx} \\n {self.tfidf_weights.iloc[:,symptom_idx]}\")\n",
    "            symptom_vector[index] = self.tfidf_weights.iloc[:,symptom_idx]\n",
    "            \n",
    "        # Populate disease vector   \n",
    "        for index, record in enumerate(list_of_records):\n",
    "            icd_code = record['ICD9_3CHAR']\n",
    "            if icd_code in self.icd9_dict:\n",
    "                \n",
    "                diagnosis_index = self.icd9_dict[icd_code]\n",
    "                diag_vector[diagnosis_index] = 1\n",
    "                # print(f\"icd code : {icd_code}, diagnosis_index : {diagnosis_index}\")\n",
    "        return torch.tensor(symptom_vector, dtype=torch.float), torch.tensor(diag_vector, dtype=torch.float), len(symptom_list)\n",
    "    \n",
    "    def create_symptom_vector(self, symptoms, filtered_symptom_dict):\n",
    "        symp_index_list = []\n",
    "        symp_list = str(symptoms).split(\"|\")    \n",
    "        # only consider notes with symptoms count more than 1\n",
    "        if len(symp_list) > 1:\n",
    "            for symptom in symp_list[:50]:\n",
    "                if symptom in filtered_symptom_dict:\n",
    "                    symp_index_list.append(filtered_symptom_dict[symptom])\n",
    "        return symp_index_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0593129",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(f\"data/top_N_filtered{tag}.csv\")\n",
    "train_size = int(len(dataset)*0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=400, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da02b006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size : 44047\n",
      "symptom_item : torch.Size([50, 50])\n",
      "diag_item : tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Dataset size : {len(dataset)}\")\n",
    "symptom_item, diag_item, symptom_len = dataset[8]\n",
    "print(f\"symptom_item : {symptom_item.shape}\")\n",
    "print(f\"diag_item : {diag_item}\")\n",
    "symptom_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b9204a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8810"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019be4e",
   "metadata": {},
   "source": [
    "<h3> Define BiLSRM Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd13d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiseaseSymptomLstm(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(DiseaseSymptomLstm, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=0.8)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, symp_length):\n",
    "        # h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "        # c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "        # print(f\"x.shape : {x.shape}\") \n",
    "        # x.shape : torch.Size([32, 50, 50])\n",
    "        \n",
    "        out, (ht, ct) = self.bilstm(x)\n",
    "        \n",
    "        # print(f\"out.shape : {out.shape}, ht.shape : {ht.shape}, ct.shape : {ct.shape}\")\n",
    "        # returning last layer of output from hidden state\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = DiseaseSymptomLstm(50, 100, 2, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376364b3",
   "metadata": {},
   "source": [
    "<h3> Training and inferencing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "271093ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cef6667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "DISEASE_THRESHOLD = 0.20\n",
    "\n",
    "def eval(model, test_loader):\n",
    "    \n",
    "    \"\"\"    \n",
    "    INPUT:\n",
    "        model: model\n",
    "        test_loader: dataloader\n",
    "        \n",
    "    OUTPUT:\n",
    "        precision: overall micro precision score\n",
    "        recall: overall micro recall score\n",
    "        f1: overall micro f1 score\n",
    "        \n",
    "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for sequences, labels, symp_len in test_loader:\n",
    "        # your code here\n",
    "        y_prob = model(sequences, symp_len)\n",
    "        y_hat = (y_prob > DISEASE_THRESHOLD).int()\n",
    "        # print(f\"y_prob: {y_hat}\")\n",
    "        # print(f\"labels: {labels}\")\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, labels.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    return p, r, f, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6133d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.402175\n",
      "Epoch: 1 \t Validation p: 0.29, r:0.43, f: 0.34, auc: 0.64\n",
      "Epoch: 2 \t Training Loss: 0.341957\n",
      "Epoch: 2 \t Validation p: 0.32, r:0.50, f: 0.39, auc: 0.67\n",
      "Epoch: 3 \t Training Loss: 0.324790\n",
      "Epoch: 3 \t Validation p: 0.34, r:0.50, f: 0.41, auc: 0.68\n",
      "Epoch: 4 \t Training Loss: 0.315602\n",
      "Epoch: 4 \t Validation p: 0.36, r:0.50, f: 0.42, auc: 0.68\n",
      "Epoch: 5 \t Training Loss: 0.306922\n",
      "Epoch: 5 \t Validation p: 0.39, r:0.50, f: 0.44, auc: 0.69\n",
      "Epoch: 6 \t Training Loss: 0.301950\n",
      "Epoch: 6 \t Validation p: 0.38, r:0.53, f: 0.44, auc: 0.70\n",
      "Epoch: 7 \t Training Loss: 0.299680\n",
      "Epoch: 7 \t Validation p: 0.38, r:0.53, f: 0.44, auc: 0.70\n",
      "Epoch: 8 \t Training Loss: 0.298493\n",
      "Epoch: 8 \t Validation p: 0.41, r:0.49, f: 0.45, auc: 0.69\n",
      "Epoch: 9 \t Training Loss: 0.297255\n",
      "Epoch: 9 \t Validation p: 0.39, r:0.52, f: 0.45, auc: 0.70\n",
      "Epoch: 10 \t Training Loss: 0.295887\n",
      "Epoch: 10 \t Validation p: 0.39, r:0.53, f: 0.45, auc: 0.70\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"    \n",
    "    INPUT:\n",
    "        model: the model\n",
    "        train_loader: dataloder\n",
    "        val_loader: dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sequences, y_true, symp_len in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # print(f\"sequence.shape {sequences.shape}, len: {symp_len}\")\n",
    "            y_hat = model(sequences, symp_len)\n",
    "#             print(f\"y_hat : {y_hat}\")\n",
    "#             print(f\"y_true : {y_true}\")\n",
    "            \n",
    "            loss = criterion(y_hat, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, auc = eval(model, test_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, auc: {:.2f}'.format(epoch+1, p, r, f, auc))\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "train(model, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe375d1",
   "metadata": {},
   "source": [
    "n_epochs = 2, learning rate = 0.001 <br>\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.332956\n",
    "Epoch: 1 \t Validation p: 0.35, r:0.51, f: 0.42, auc: 0.69\n",
    "Epoch: 2 \t Training Loss: 0.303654\n",
    "Epoch: 2 \t Validation p: 0.40, r:0.51, f: 0.45, auc: 0.70\n",
    "</code>\n",
    "         <code>\n",
    "Epoch: 1 \t Training Loss: 0.298307\n",
    "Epoch: 1 \t Validation p: 0.40, r:0.53, f: 0.45, auc: 0.71\n",
    "Epoch: 2 \t Training Loss: 0.294352\n",
    "Epoch: 2 \t Validation p: 0.38, r:0.57, f: 0.46, auc: 0.72\n",
    "Epoch: 3 \t Training Loss: 0.290162\n",
    "Epoch: 3 \t Validation p: 0.40, r:0.55, f: 0.46, auc: 0.72\n",
    "Epoch: 4 \t Training Loss: 0.286585\n",
    "Epoch: 4 \t Validation p: 0.39, r:0.57, f: 0.47, auc: 0.72\n",
    "Epoch: 5 \t Training Loss: 0.283719\n",
    "Epoch: 5 \t Validation p: 0.40, r:0.58, f: 0.47, auc: 0.73      \n",
    "    </code>\n",
    "\n",
    "<h5>n_epochs=5, learning_rate = 0.005</h5>\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.292470\n",
    "Epoch: 1 \t Validation p: 0.36, r:0.59, f: 0.45, auc: 0.72\n",
    "Epoch: 2 \t Training Loss: 0.288039\n",
    "Epoch: 2 \t Validation p: 0.37, r:0.62, f: 0.46, auc: 0.73\n",
    "Epoch: 3 \t Training Loss: 0.284519\n",
    "Epoch: 3 \t Validation p: 0.38, r:0.60, f: 0.47, auc: 0.73\n",
    "Epoch: 4 \t Training Loss: 0.282809\n",
    "Epoch: 4 \t Validation p: 0.37, r:0.63, f: 0.46, auc: 0.74\n",
    "Epoch: 5 \t Training Loss: 0.281407\n",
    "Epoch: 5 \t Validation p: 0.38, r:0.61, f: 0.47, auc: 0.74\n",
    "</code>\n",
    "\n",
    "With Batch size of 400, starting fresh!\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.400608\n",
    "Epoch: 1 \t Validation p: 0.28, r:0.40, f: 0.33, auc: 0.63\n",
    "Epoch: 2 \t Training Loss: 0.347110\n",
    "Epoch: 2 \t Validation p: 0.35, r:0.36, f: 0.35, auc: 0.63\n",
    "Epoch: 3 \t Training Loss: 0.326768\n",
    "Epoch: 3 \t Validation p: 0.37, r:0.44, f: 0.40, auc: 0.67\n",
    "Epoch: 4 \t Training Loss: 0.319100\n",
    "Epoch: 4 \t Validation p: 0.37, r:0.46, f: 0.41, auc: 0.67\n",
    "Epoch: 5 \t Training Loss: 0.312817\n",
    "Epoch: 5 \t Validation p: 0.36, r:0.54, f: 0.43, auc: 0.70\n",
    "    \n",
    "Epoch: 1 \t Training Loss: 0.304074\n",
    "Epoch: 1 \t Validation p: 0.38, r:0.52, f: 0.44, auc: 0.70\n",
    "Epoch: 2 \t Training Loss: 0.300410\n",
    "Epoch: 2 \t Validation p: 0.39, r:0.51, f: 0.44, auc: 0.70\n",
    "Epoch: 3 \t Training Loss: 0.299053\n",
    "Epoch: 3 \t Validation p: 0.38, r:0.54, f: 0.45, auc: 0.71\n",
    "Epoch: 4 \t Training Loss: 0.297172\n",
    "Epoch: 4 \t Validation p: 0.40, r:0.52, f: 0.45, auc: 0.70\n",
    "Epoch: 5 \t Training Loss: 0.294421\n",
    "Epoch: 5 \t Validation p: 0.38, r:0.56, f: 0.45, auc: 0.71\n",
    "</code>\n",
    "\n",
    "Number of layers in BiLSRT as 4. Very slow as well!\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.691968\n",
    "Epoch: 1 \t Validation p: 0.13, r:1.00, f: 0.23, auc: 0.50\n",
    "Epoch: 2 \t Training Loss: 0.691965\n",
    "Epoch: 2 \t Validation p: 0.13, r:1.00, f: 0.23, auc: 0.50\n",
    "</code>\n",
    "\n",
    "Number of layers in BiLSRT as 3. Very slow as well!\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.329327\n",
    "Epoch: 1 \t Validation p: 0.30, r:0.56, f: 0.39, auc: 0.69\n",
    "Epoch: 2 \t Training Loss: 0.320431\n",
    "Epoch: 2 \t Validation p: 0.33, r:0.53, f: 0.41, auc: 0.69\n",
    "Epoch: 3 \t Training Loss: 0.317443\n",
    "Epoch: 3 \t Validation p: 0.35, r:0.49, f: 0.41, auc: 0.68\n",
    "Epoch: 4 \t Training Loss: 0.315925\n",
    "Epoch: 4 \t Validation p: 0.34, r:0.52, f: 0.41, auc: 0.69\n",
    "Epoch: 5 \t Training Loss: 0.311251\n",
    "Epoch: 5 \t Validation p: 0.38, r:0.49, f: 0.43, auc: 0.69\n",
    "</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
