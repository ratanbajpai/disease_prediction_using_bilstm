{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0125c6",
   "metadata": {},
   "source": [
    "<h3> Training BiLSTM Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4978a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8035f",
   "metadata": {},
   "source": [
    "<h4> Loading all the data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8834a243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4016</th>\n",
       "      <th>4017</th>\n",
       "      <th>4018</th>\n",
       "      <th>4019</th>\n",
       "      <th>4020</th>\n",
       "      <th>4021</th>\n",
       "      <th>4022</th>\n",
       "      <th>4023</th>\n",
       "      <th>4024</th>\n",
       "      <th>4025</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.389982</td>\n",
       "      <td>0.791002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087657</td>\n",
       "      <td>0.112317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.370221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341682</td>\n",
       "      <td>0.364710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107335</td>\n",
       "      <td>0.166241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.880208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.237425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477639</td>\n",
       "      <td>0.916720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.567708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.191147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.095405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.108652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198569</td>\n",
       "      <td>0.184110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.316901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409660</td>\n",
       "      <td>0.569560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.807292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.066398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.066050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.033199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055456</td>\n",
       "      <td>0.081685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.328974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.200383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.088531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141324</td>\n",
       "      <td>0.101787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241503</td>\n",
       "      <td>0.438098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.075453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075134</td>\n",
       "      <td>0.192406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.545272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.497317</td>\n",
       "      <td>0.633057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.087525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094812</td>\n",
       "      <td>0.179643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101968</td>\n",
       "      <td>0.152840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.152918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121646</td>\n",
       "      <td>0.150925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035778</td>\n",
       "      <td>0.043714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.032193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066190</td>\n",
       "      <td>0.132419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.162978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.620932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.099598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.223614</td>\n",
       "      <td>0.181238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.027163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.094767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028623</td>\n",
       "      <td>0.059987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.141851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.280859</td>\n",
       "      <td>0.484684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.270624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295170</td>\n",
       "      <td>0.490428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.065412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078712</td>\n",
       "      <td>0.120294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 4026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1    2         3         4    5    6    7    8         9  ...  \\\n",
       "0   1.000000  0.0  0.0  1.000000  0.742821  0.0  0.0  0.0  0.0  0.890625  ...   \n",
       "1   0.422535  0.0  0.0  0.389982  0.791002  0.0  0.0  0.0  0.0  0.640625  ...   \n",
       "2   0.046278  0.0  0.0  0.087657  0.112317  0.0  0.0  0.0  0.0  0.114583  ...   \n",
       "3   0.370221  0.0  0.0  0.341682  0.364710  0.0  0.0  0.0  0.0  0.437500  ...   \n",
       "4   0.039235  0.0  0.0  0.107335  0.166241  0.0  0.0  0.0  0.0  0.880208  ...   \n",
       "5   0.237425  0.0  0.0  0.477639  0.916720  0.0  0.0  0.0  0.0  0.567708  ...   \n",
       "6   0.197183  0.0  0.0  0.395349  1.000000  0.0  0.0  0.0  0.0  0.609375  ...   \n",
       "7   0.191147  0.0  0.0  0.279070  0.095405  0.0  0.0  0.0  0.0  0.145833  ...   \n",
       "8   0.108652  0.0  0.0  0.198569  0.184110  0.0  0.0  0.0  0.0  0.203125  ...   \n",
       "9   0.316901  0.0  0.0  0.409660  0.569560  0.0  0.0  0.0  0.0  0.807292  ...   \n",
       "10  0.066398  0.0  0.0  0.076923  0.066050  0.0  0.0  0.0  0.0  0.072917  ...   \n",
       "11  0.033199  0.0  0.0  0.055456  0.081685  0.0  0.0  0.0  0.0  0.145833  ...   \n",
       "12  0.328974  0.0  0.0  0.348837  0.200383  0.0  0.0  0.0  0.0  0.812500  ...   \n",
       "13  0.088531  0.0  0.0  0.141324  0.101787  0.0  0.0  0.0  0.0  0.182292  ...   \n",
       "14  0.140845  0.0  0.0  0.241503  0.438098  0.0  0.0  0.0  0.0  0.604167  ...   \n",
       "15  0.075453  0.0  0.0  0.075134  0.192406  0.0  0.0  0.0  0.0  0.229167  ...   \n",
       "16  0.545272  0.0  0.0  0.497317  0.633057  0.0  0.0  0.0  0.0  1.000000  ...   \n",
       "17  0.087525  0.0  0.0  0.094812  0.179643  0.0  0.0  0.0  0.0  0.265625  ...   \n",
       "18  0.042254  0.0  0.0  0.101968  0.152840  0.0  0.0  0.0  0.0  0.296875  ...   \n",
       "19  0.152918  0.0  0.0  0.121646  0.150925  0.0  0.0  0.0  0.0  0.255208  ...   \n",
       "20  0.010060  0.0  0.0  0.035778  0.043714  0.0  0.0  0.0  0.0  0.052083  ...   \n",
       "21  0.032193  0.0  0.0  0.066190  0.132419  0.0  0.0  0.0  0.0  0.114583  ...   \n",
       "22  0.162978  0.0  0.0  0.300537  0.620932  0.0  0.0  0.0  0.0  0.734375  ...   \n",
       "23  0.099598  0.0  0.0  0.223614  0.181238  0.0  0.0  0.0  0.0  0.302083  ...   \n",
       "24  0.027163  0.0  0.0  0.069767  0.094767  0.0  0.0  0.0  0.0  0.041667  ...   \n",
       "25  0.015091  0.0  0.0  0.028623  0.059987  0.0  0.0  0.0  0.0  0.052083  ...   \n",
       "26  0.141851  0.0  0.0  0.280859  0.484684  0.0  0.0  0.0  0.0  0.640625  ...   \n",
       "27  0.270624  0.0  0.0  0.295170  0.490428  0.0  0.0  0.0  0.0  0.625000  ...   \n",
       "28  0.009054  0.0  0.0  0.016100  0.065412  0.0  0.0  0.0  0.0  0.109375  ...   \n",
       "29  0.029175  0.0  0.0  0.078712  0.120294  0.0  0.0  0.0  0.0  0.109375  ...   \n",
       "\n",
       "     4016      4017   4018  4019      4020      4021  4022   4023  4024  4025  \n",
       "0   1.000  0.500000  0.500   0.0  0.666667  1.000000   0.2  0.250  0.50  0.50  \n",
       "1   0.375  0.166667  0.500   0.5  0.500000  0.666667   0.6  0.000  1.00  0.00  \n",
       "2   0.000  0.000000  0.000   0.0  0.166667  0.000000   0.2  0.000  0.00  0.00  \n",
       "3   0.250  0.000000  0.125   0.0  0.333333  0.333333   0.6  0.000  0.25  0.25  \n",
       "4   0.250  0.000000  0.375   0.5  0.166667  0.333333   0.0  0.000  0.00  0.25  \n",
       "5   0.625  0.166667  0.125   0.0  0.166667  0.000000   0.2  0.125  0.25  0.00  \n",
       "6   0.250  0.166667  0.125   0.5  0.000000  0.333333   0.2  0.125  0.25  0.00  \n",
       "7   0.000  0.166667  0.000   0.0  0.000000  1.000000   0.0  0.000  0.00  0.00  \n",
       "8   0.000  0.166667  0.000   0.0  0.166667  0.000000   0.0  0.000  0.00  0.25  \n",
       "9   0.375  0.166667  0.500   0.0  0.666667  0.333333   0.8  0.125  0.00  1.00  \n",
       "10  0.000  0.000000  0.000   0.0  0.166667  0.000000   0.2  0.000  0.00  0.00  \n",
       "11  0.125  0.000000  0.000   0.0  0.166667  0.000000   0.0  0.000  0.00  0.00  \n",
       "12  0.250  0.333333  1.000   1.0  0.333333  0.000000   0.0  1.000  1.00  0.75  \n",
       "13  0.000  0.000000  0.000   0.0  0.000000  0.000000   0.0  0.000  0.50  0.50  \n",
       "14  0.125  0.500000  0.625   0.5  0.500000  0.333333   0.8  0.125  0.75  0.25  \n",
       "15  0.125  0.000000  0.125   0.0  0.333333  0.000000   0.2  0.000  0.25  0.75  \n",
       "16  0.375  0.000000  0.875   0.0  1.000000  1.000000   0.6  0.000  0.75  0.25  \n",
       "17  0.000  0.000000  0.125   0.5  0.333333  0.000000   0.8  0.000  0.25  0.00  \n",
       "18  0.500  0.000000  0.125   0.0  0.166667  0.333333   0.2  0.250  0.00  0.25  \n",
       "19  0.125  0.000000  0.250   0.0  0.333333  0.333333   0.6  0.250  0.00  0.00  \n",
       "20  0.000  0.000000  0.250   0.0  0.166667  0.000000   0.6  0.000  0.25  0.00  \n",
       "21  0.250  0.000000  0.250   0.0  0.000000  0.000000   0.2  0.125  0.25  0.00  \n",
       "22  0.500  0.000000  0.250   0.5  0.166667  0.333333   0.0  0.500  0.00  0.00  \n",
       "23  0.375  1.000000  0.250   0.0  0.000000  1.000000   0.0  0.000  0.25  0.00  \n",
       "24  0.125  0.166667  0.125   0.0  0.000000  0.000000   0.2  0.000  0.00  0.00  \n",
       "25  0.000  0.000000  0.000   0.5  0.000000  0.000000   0.0  0.000  0.00  0.00  \n",
       "26  0.125  0.166667  0.500   0.5  0.166667  0.333333   0.0  0.250  0.75  0.25  \n",
       "27  0.250  0.333333  0.875   0.5  0.000000  0.666667   0.0  0.625  0.50  0.25  \n",
       "28  0.000  0.000000  0.000   0.0  0.000000  0.000000   0.0  0.125  0.00  0.00  \n",
       "29  0.250  0.166667  0.000   0.0  0.000000  0.000000   0.0  0.000  0.25  0.00  \n",
       "\n",
       "[30 rows x 4026 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = \"_100_v3\"\n",
    "disease_symptoms = pd.read_csv(f\"../data/top_N_filtered{tag}.csv\")\n",
    "with open(f\"../data/filtered_symptom_dict{tag}.csv\", 'r') as f:\n",
    "    symptom_dict = data = json.load(f)\n",
    "\n",
    "with open(f\"../data/icd9_dict{tag}.csv\", 'r') as f:\n",
    "    icd9_dict = json.load(f)\n",
    "\n",
    "tfidf_weights = pd.read_csv(f\"../data/weight_i_j_norm{tag}.csv\")\n",
    "tfidf_weights.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf45e64",
   "metadata": {},
   "source": [
    "<h3> Define Custom DataLoader </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "18fc365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_SYMPTOMS = 50\n",
    "MAX_DISEASE = 100\n",
    "BATCH_SIZE=400\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filename):        \n",
    "        self.hadm_id_map = {}\n",
    "        \n",
    "        # TF-IDF Weights\n",
    "        self.tfidf_weights = pd.read_csv(f\"../data/weight_i_j{tag}.csv\")\n",
    "        \n",
    "        # Symptom dictionary\n",
    "        with open(f\"../data/filtered_symptom_dict{tag}.csv\", 'r') as f:\n",
    "            self.symptom_dict = json.load(f)\n",
    "        \n",
    "        with open(f\"../data/icd9_dict{tag}.csv\", 'r') as f:\n",
    "            self.icd9_dict = json.load(f)\n",
    "        \n",
    "        # read in the data files\n",
    "        self.hadm_list, self.hadm_id_map = self.process_raw_data(filename)\n",
    "        \n",
    "        \n",
    "    def process_raw_data(self, filename):\n",
    "        symptom_disease_data = pd.read_csv(filename)\n",
    "        hadm_list = []\n",
    "        hadm_id_map = {}\n",
    "        # Collecting all records for one admission in one list\n",
    "        for index, record in symptom_disease_data.iterrows():\n",
    "            # print(f\"processing {index} - {record}\")\n",
    "            hadm_id = record['HADM_ID']            \n",
    "            if hadm_id not in hadm_id_map:\n",
    "                # Also parse symptom text and convert it to symptom vector\n",
    "                symptom_text = record['SYMPTOMS'] \n",
    "                symp_vec = self.create_symptom_vector(symptom_text, self.symptom_dict)\n",
    "                \n",
    "                # Only process record if number of symptoms are more than 1\n",
    "                if len(symp_vec) > 1:\n",
    "                    # Create a tuple of HADM IDs and symptom vector\n",
    "                    hadm_id_map[hadm_id] = tuple(([], symp_vec))\n",
    "                    hadm_list.append(hadm_id)\n",
    "                else:\n",
    "                    # If number of symptoms is less than or equal to 1, ignore this record\n",
    "                    continue         \n",
    "\n",
    "            # HADM ID is already added in the map. Just add record to the tuple\n",
    "            hadm_id_map[hadm_id][0].append(record)\n",
    "        \n",
    "        return hadm_list, hadm_id_map\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hadm_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            Output:\n",
    "            symptpm_vector : max_number_of_symptoms (50) x number_of_diagnoses (50)\n",
    "            diagnoses_vector = number_of_diagnoses\n",
    "            symptom_count = number of symtoms for current record\n",
    "        \"\"\"\n",
    "        hadm_id = self.hadm_list[index]\n",
    "        list_of_records, symptom_list = self.hadm_id_map[hadm_id]\n",
    "        \n",
    "        # Create symptom vector for this admission record\n",
    "        symptom_vector = np.zeros((MAX_SYMPTOMS, MAX_DISEASE))\n",
    "        \n",
    "        # Create Diagnosis vector to keep true labels\n",
    "        diag_vector = np.zeros((MAX_DISEASE))\n",
    "\n",
    "        # Populate Symptom Vector by getting corresponding embeddings from TF-IDF vector\n",
    "        for index, symptom_idx in enumerate(symptom_list):\n",
    "            # print(f\"Symptom vector index: {index}, symptom index : {symptom_idx} \\n {self.tfidf_weights.iloc[:,symptom_idx]}\")\n",
    "            symptom_vector[index] = self.tfidf_weights.iloc[:,symptom_idx]\n",
    "            \n",
    "        # Populate disease vector   \n",
    "        for index, record in enumerate(list_of_records):\n",
    "            icd_code = record['ICD9_3CHAR']\n",
    "            if icd_code in self.icd9_dict:\n",
    "                diagnosis_index = self.icd9_dict[icd_code]\n",
    "                diag_vector[diagnosis_index] = 1\n",
    "                # print(f\"icd code : {icd_code}, diagnosis_index : {diagnosis_index}\")\n",
    "        return torch.tensor(symptom_vector.T, dtype=torch.float), torch.tensor(diag_vector, dtype=torch.float), len(symptom_list)\n",
    "    \n",
    "    def create_symptom_vector(self, symptoms, filtered_symptom_dict):\n",
    "        symp_index_list = []\n",
    "        symp_list = str(symptoms).split(\"|\")    \n",
    "        # only consider notes with symptoms count more than 1\n",
    "        if len(symp_list) > 1:\n",
    "            for symptom in symp_list:\n",
    "                if symptom in filtered_symptom_dict:\n",
    "                    symp_index_list.append(filtered_symptom_dict[symptom])\n",
    "        # print(f\"symp_index_list[:MAX_SYMPTOMS] -- {symp_index_list[:MAX_SYMPTOMS]}\")\n",
    "        return symp_index_list[:MAX_SYMPTOMS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a0593129",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(f\"../data/top_N_filtered{tag}.csv\")\n",
    "# train_size = int(len(dataset)*0.8)\n",
    "# test_size = int(len(dataset)*0.2)\n",
    "train_size = 20000\n",
    "test_size = 2000\n",
    "validation_size = len(dataset)  - (train_size + test_size)\n",
    "train_dataset, test_dataset, validation_dataset = torch.utils.data.random_split(dataset, [train_size, test_size, validation_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "da02b006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size : 50374\n",
      "symptom_item : torch.Size([100, 50])\n",
      "diag_item : tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.3926, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [3.2030, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.1393, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.4178, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.4178, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.4178, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Dataset size : {len(dataset)}\")\n",
    "symptom_item, diag_item, symptom_len = dataset[8]\n",
    "print(f\"symptom_item : {symptom_item.shape}\")\n",
    "print(f\"diag_item : {diag_item}\")\n",
    "symptom_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8b9204a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019be4e",
   "metadata": {},
   "source": [
    "<h3> Define BiLSRM Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fd13d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiseaseSymptomLstm(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, batch_size):\n",
    "        super(DiseaseSymptomLstm, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=0.8)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, symp_length):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
    "        # h0, c0 = self.init_hidden(x)\n",
    "\n",
    "        # x = torch.nn.utils.rnn.pack_padded_sequence(x, symp_length, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        out, (ht, ct) = self.bilstm(x, (h0, c0))\n",
    "        \n",
    "        # out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        # print(f\"out.shape : {out.shape}, ht.shape : {ht.shape}, ct.shape : {ct.shape}\")\n",
    "        # returning last layer of output from hidden state\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = DiseaseSymptomLstm(MAX_SYMPTOMS, 100, 2, MAX_DISEASE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376364b3",
   "metadata": {},
   "source": [
    "<h3> Training and inferencing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "271093ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a649a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_padded_pred_and_true_labels(y_pred, y_true, symptom_length_vector):\n",
    "    \n",
    "    print(f\"y_pred.shape: {y_pred.shape}, y_true.shape: {y_true.shape}, symptom_length_vector: {symptom_length_vector.shape}\")\n",
    "    # Create a mask which will have all padded field to be zero\n",
    "    \n",
    "    mask_vector = np.ones(y_pred.shape)\n",
    "    idx = 0\n",
    "    for symptom_length in symptom_length_vector:\n",
    "        mask_vector[idx,symptom_length:] = 0\n",
    "        idx += 1\n",
    "        \n",
    "    mask = torch.tensor(mask_vector)\n",
    "    mask_1 = mask.view(-1)\n",
    "    mask_1 = mask_1.ge(1)\n",
    "    \n",
    "    y_pred_1 = y_pred.view(-1)\n",
    "    y_true_1 = y_true.view(-1)\n",
    "    \n",
    "    y_pred_final = torch.masked_select(y_pred_1, mask_1)\n",
    "    y_true_final = torch.masked_select(y_true_1, mask_1)\n",
    "    \n",
    "    print(f\"y_pred_final.shape: {y_pred_final.shape}, y_true_final.shape: {y_true_final.shape}\")\n",
    "    return y_pred_final, y_true_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5fced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cef6667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "DISEASE_THRESHOLD = 0.20\n",
    "\n",
    "def eval(model, test_loader):\n",
    "    \n",
    "    \"\"\"    \n",
    "    INPUT:\n",
    "        model: model\n",
    "        test_loader: dataloader\n",
    "        \n",
    "    OUTPUT:\n",
    "        precision: overall micro precision score\n",
    "        recall: overall micro recall score\n",
    "        f1: overall micro f1 score\n",
    "        \n",
    "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    for sequences, labels, symp_len in test_loader:\n",
    "        # your code here\n",
    "        y_prob = model(sequences, symp_len)\n",
    "        y_hat = (y_prob > DISEASE_THRESHOLD).int()\n",
    "        # print(f\"y_prob: {y_hat}\")\n",
    "        # print(f\"labels: {labels}\")\n",
    "        #y_hat, labels = get_non_padded_pred_and_true_labels(y_hat.detach().to('cpu'), labels.detach().to('cpu'), symp_len)\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, labels.detach().to('cpu')), dim=0)\n",
    "        \n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    return p, r, f, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6133d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.236686\n",
      "Epoch: 1 \t Validation p: 0.34, r:0.39, f: 0.36, auc: 0.66\n",
      "Epoch: 2 \t Training Loss: 0.235472\n",
      "Epoch: 2 \t Validation p: 0.34, r:0.40, f: 0.37, auc: 0.66\n",
      "Epoch: 3 \t Training Loss: 0.233751\n",
      "Epoch: 3 \t Validation p: 0.35, r:0.39, f: 0.37, auc: 0.66\n",
      "Epoch: 4 \t Training Loss: 0.232628\n",
      "Epoch: 4 \t Validation p: 0.34, r:0.39, f: 0.37, auc: 0.66\n",
      "Epoch: 5 \t Training Loss: 0.232200\n",
      "Epoch: 5 \t Validation p: 0.35, r:0.39, f: 0.37, auc: 0.66\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"    \n",
    "    INPUT:\n",
    "        model: the model\n",
    "        train_loader: dataloder\n",
    "        val_loader: dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sequences, y_true, symp_len in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            #print(f\"sequence.shape {sequences.shape}, symp_len.shape: {symp_len.shape}\")\n",
    "            y_hat = model(sequences, symp_len)\n",
    "            \n",
    "            #y_hat, y_true = get_non_padded_pred_and_true_labels(y_hat, y_true, symp_len)\n",
    "            loss = criterion(y_hat, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, auc = eval(model, test_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, auc: {:.2f}'.format(epoch+1, p, r, f, auc))\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "\n",
    "train(model, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe375d1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "_100_v3 - 10 Epochs - Using non-normalized weights. lr=0.001\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.236686\n",
    "Epoch: 1 \t Validation p: 0.34, r:0.39, f: 0.36, auc: 0.66\n",
    "Epoch: 2 \t Training Loss: 0.235472\n",
    "Epoch: 2 \t Validation p: 0.34, r:0.40, f: 0.37, auc: 0.66\n",
    "Epoch: 3 \t Training Loss: 0.233751\n",
    "Epoch: 3 \t Validation p: 0.35, r:0.39, f: 0.37, auc: 0.66\n",
    "Epoch: 4 \t Training Loss: 0.232628\n",
    "Epoch: 4 \t Validation p: 0.34, r:0.39, f: 0.37, auc: 0.66\n",
    "Epoch: 5 \t Training Loss: 0.232200\n",
    "Epoch: 5 \t Validation p: 0.35, r:0.39, f: 0.37, auc: 0.66\n",
    "</code>\n",
    "\n",
    "----------------\n",
    "\n",
    "n_epochs = 2, learning rate = 0.001 <br>\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.332956\n",
    "Epoch: 1 \t Validation p: 0.35, r:0.51, f: 0.42, auc: 0.69\n",
    "Epoch: 2 \t Training Loss: 0.303654\n",
    "Epoch: 2 \t Validation p: 0.40, r:0.51, f: 0.45, auc: 0.70\n",
    "</code>\n",
    "         <code>\n",
    "Epoch: 1 \t Training Loss: 0.298307\n",
    "Epoch: 1 \t Validation p: 0.40, r:0.53, f: 0.45, auc: 0.71\n",
    "Epoch: 2 \t Training Loss: 0.294352\n",
    "Epoch: 2 \t Validation p: 0.38, r:0.57, f: 0.46, auc: 0.72\n",
    "Epoch: 3 \t Training Loss: 0.290162\n",
    "Epoch: 3 \t Validation p: 0.40, r:0.55, f: 0.46, auc: 0.72\n",
    "Epoch: 4 \t Training Loss: 0.286585\n",
    "Epoch: 4 \t Validation p: 0.39, r:0.57, f: 0.47, auc: 0.72\n",
    "Epoch: 5 \t Training Loss: 0.283719\n",
    "Epoch: 5 \t Validation p: 0.40, r:0.58, f: 0.47, auc: 0.73      \n",
    "    </code>\n",
    "\n",
    "<h5>n_epochs=5, learning_rate = 0.005</h5>\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.292470\n",
    "Epoch: 1 \t Validation p: 0.36, r:0.59, f: 0.45, auc: 0.72\n",
    "Epoch: 2 \t Training Loss: 0.288039\n",
    "Epoch: 2 \t Validation p: 0.37, r:0.62, f: 0.46, auc: 0.73\n",
    "Epoch: 3 \t Training Loss: 0.284519\n",
    "Epoch: 3 \t Validation p: 0.38, r:0.60, f: 0.47, auc: 0.73\n",
    "Epoch: 4 \t Training Loss: 0.282809\n",
    "Epoch: 4 \t Validation p: 0.37, r:0.63, f: 0.46, auc: 0.74\n",
    "Epoch: 5 \t Training Loss: 0.281407\n",
    "Epoch: 5 \t Validation p: 0.38, r:0.61, f: 0.47, auc: 0.74\n",
    "</code>\n",
    "\n",
    "With Batch size of 400, starting fresh!\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.400608\n",
    "Epoch: 1 \t Validation p: 0.28, r:0.40, f: 0.33, auc: 0.63\n",
    "Epoch: 2 \t Training Loss: 0.347110\n",
    "Epoch: 2 \t Validation p: 0.35, r:0.36, f: 0.35, auc: 0.63\n",
    "Epoch: 3 \t Training Loss: 0.326768\n",
    "Epoch: 3 \t Validation p: 0.37, r:0.44, f: 0.40, auc: 0.67\n",
    "Epoch: 4 \t Training Loss: 0.319100\n",
    "Epoch: 4 \t Validation p: 0.37, r:0.46, f: 0.41, auc: 0.67\n",
    "Epoch: 5 \t Training Loss: 0.312817\n",
    "Epoch: 5 \t Validation p: 0.36, r:0.54, f: 0.43, auc: 0.70\n",
    "    \n",
    "Epoch: 1 \t Training Loss: 0.304074\n",
    "Epoch: 1 \t Validation p: 0.38, r:0.52, f: 0.44, auc: 0.70\n",
    "Epoch: 2 \t Training Loss: 0.300410\n",
    "Epoch: 2 \t Validation p: 0.39, r:0.51, f: 0.44, auc: 0.70\n",
    "Epoch: 3 \t Training Loss: 0.299053\n",
    "Epoch: 3 \t Validation p: 0.38, r:0.54, f: 0.45, auc: 0.71\n",
    "Epoch: 4 \t Training Loss: 0.297172\n",
    "Epoch: 4 \t Validation p: 0.40, r:0.52, f: 0.45, auc: 0.70\n",
    "Epoch: 5 \t Training Loss: 0.294421\n",
    "Epoch: 5 \t Validation p: 0.38, r:0.56, f: 0.45, auc: 0.71\n",
    "</code>\n",
    "\n",
    "Number of layers in BiLSRT as 4. Very slow as well!\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.691968\n",
    "Epoch: 1 \t Validation p: 0.13, r:1.00, f: 0.23, auc: 0.50\n",
    "Epoch: 2 \t Training Loss: 0.691965\n",
    "Epoch: 2 \t Validation p: 0.13, r:1.00, f: 0.23, auc: 0.50\n",
    "</code>\n",
    "\n",
    "Number of layers in BiLSRT as 3. Very slow as well!\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.329327\n",
    "Epoch: 1 \t Validation p: 0.30, r:0.56, f: 0.39, auc: 0.69\n",
    "Epoch: 2 \t Training Loss: 0.320431\n",
    "Epoch: 2 \t Validation p: 0.33, r:0.53, f: 0.41, auc: 0.69\n",
    "Epoch: 3 \t Training Loss: 0.317443\n",
    "Epoch: 3 \t Validation p: 0.35, r:0.49, f: 0.41, auc: 0.68\n",
    "Epoch: 4 \t Training Loss: 0.315925\n",
    "Epoch: 4 \t Validation p: 0.34, r:0.52, f: 0.41, auc: 0.69\n",
    "Epoch: 5 \t Training Loss: 0.311251\n",
    "Epoch: 5 \t Validation p: 0.38, r:0.49, f: 0.43, auc: 0.69\n",
    "</code>\n",
    "\n",
    "10 Epoch run with 50 disease\n",
    "<code>\n",
    "Epoch: 1 \t Training Loss: 0.403777\n",
    "Epoch: 1 \t Validation p: 0.28, r:0.44, f: 0.35, auc: 0.64\n",
    "Epoch: 2 \t Training Loss: 0.354655\n",
    "Epoch: 2 \t Validation p: 0.33, r:0.42, f: 0.37, auc: 0.65\n",
    "Epoch: 3 \t Training Loss: 0.343463\n",
    "Epoch: 3 \t Validation p: 0.34, r:0.46, f: 0.39, auc: 0.66\n",
    "Epoch: 4 \t Training Loss: 0.337650\n",
    "Epoch: 4 \t Validation p: 0.33, r:0.49, f: 0.39, auc: 0.67\n",
    "Epoch: 5 \t Training Loss: 0.331578\n",
    "Epoch: 5 \t Validation p: 0.36, r:0.48, f: 0.41, auc: 0.68\n",
    "Epoch: 6 \t Training Loss: 0.323138\n",
    "Epoch: 6 \t Validation p: 0.36, r:0.50, f: 0.42, auc: 0.69\n",
    "Epoch: 7 \t Training Loss: 0.319433\n",
    "Epoch: 7 \t Validation p: 0.36, r:0.51, f: 0.43, auc: 0.69\n",
    "Epoch: 8 \t Training Loss: 0.317392\n",
    "Epoch: 8 \t Validation p: 0.39, r:0.48, f: 0.43, auc: 0.68\n",
    "Epoch: 9 \t Training Loss: 0.315443\n",
    "Epoch: 9 \t Validation p: 0.37, r:0.52, f: 0.43, auc: 0.69\n",
    "Epoch: 10 \t Training Loss: 0.313061\n",
    "Epoch: 10 \t Validation p: 0.38, r:0.52, f: 0.44, auc: 0.70\n",
    "<code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
